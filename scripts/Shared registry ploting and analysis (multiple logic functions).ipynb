{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3a1079",
   "metadata": {},
   "source": [
    "To launch in SuperCloud from a Computed Node\n",
    "\n",
    "\n",
    "LLsub -i full #for an exclusive node\n",
    "\n",
    "LLsub -i -s 40 #for node with 40 CPUs\n",
    "\n",
    "LLsub -i -s 40 -g volta:1 #for node with 40 CPUs and 1 Volta GPU\n",
    "\n",
    "salloc  --job-name=interactive --qos=high --time=00:60:00 --partition=debug-gpu --gres=gpu:volta:1 --cpus-per-task=40 srun    --pty bash -i\n",
    "\n",
    "salloc  --job-name=interactive --qos=high --time=00:60:00 --partition=debug-cpu --cpus-per-task=40 srun  --pty bash -i\n",
    "\n",
    "module load anaconda/2023a-pytorch\n",
    "\n",
    "jupyter lab --no-browser --ip=0.0.0.0 --port=8890\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1529eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, time\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm   \n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import re, itertools, pickle, networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import networkx as nx    \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b61cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgd.utils.utils5 import (\n",
    "    calculate_truth_table_v2,\n",
    "    energy_score,\n",
    "    check_implicit_OR_existence_v3\n",
    ")\n",
    "\n",
    "from dgd.environments.drl3env_loader5 import _apply_implicit_or, _compute_truth_key, _compute_hash, _apply_implicit_or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce6a99",
   "metadata": {},
   "source": [
    "Folder with the biological circuit designs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b148adf-3cca-41aa-8bd8-cceb66b475f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = \"/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/scripts/runs/Fig3_4input_200_logic_functions_registry_sampling_drl3env_loader5/seed_1\"\n",
    "run_dir = Path(run_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cdbd9",
   "metadata": {},
   "source": [
    "Load action space motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTIFS_PATH = \"/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/scripts/action_motifs.pkl\"\n",
    "with open(MOTIFS_PATH, \"rb\") as f:\n",
    "    action_motifs = pickle.load(f)\n",
    "    \n",
    "UNIQUE_GRAPHS = action_motifs[\"graphs\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57437ee3",
   "metadata": {},
   "source": [
    "General utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1232e-e1da-40d2-a94b-16f9e0a07f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_motif_canonicals():\n",
    "    \"\"\"\n",
    "    Compute canonical form for every motif in global UNIQUE_GRAPHS and\n",
    "    store them in UNIQUE_GRAPHS_canonical, with a progress bar.\n",
    "    \"\"\"\n",
    "    global UNIQUE_GRAPHS_canonical\n",
    "    bar = tqdm(UNIQUE_GRAPHS, desc=\"Canonicalising motifs\", unit=\"motif\")\n",
    "    UNIQUE_GRAPHS_canonical = [_apply_implicit_or(g) for g in bar]\n",
    "    print(f\"Built canonical bank for {len(UNIQUE_GRAPHS_canonical)} motifs.\")\n",
    "\n",
    "def check_vs_motif_bank(graphs):\n",
    "    \"\"\"\n",
    "    For each graph in `graphs`, report whether it is isomorphic to any\n",
    "    canonical motif.  Shows a progress bar over the input list.\n",
    "    \"\"\"\n",
    "    if \"UNIQUE_GRAPHS_canonical\" not in globals():\n",
    "        build_motif_canonicals()\n",
    "\n",
    "    bar = tqdm(enumerate(graphs, 1), total=len(graphs), desc=\"Matching\", unit=\"graph\")\n",
    "    for idx, g in bar:\n",
    "        canon_g = _apply_implicit_or(g)\n",
    "        \n",
    "        match = False                              # default: no match yet\n",
    "        for m in UNIQUE_GRAPHS_canonical:          # scan every canonical motif\n",
    "            if nx.is_isomorphic(canon_g, m):       # found an isomorphic partner?\n",
    "                match = True                       # mark it\n",
    "                break                              # stop checking further\n",
    "\n",
    "        status = \"MATCH\" if match else \"NEW  \"\n",
    "        bar.set_postfix({\"last\": status})      # live status in the bar\n",
    "        print(f\"Graph {idx:>2}: {status}   \"\n",
    "              f\"nodes={g.number_of_nodes():>2}  edges={g.number_of_edges():>2}\")\n",
    "\n",
    "def load_registry(pkl_file):\n",
    "    \"\"\"Load the pickle and rebuild NetworkX graphs.\"\"\"\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        saved = pickle.load(f)\n",
    "\n",
    "    registry = {}\n",
    "    for h, bucket in saved.items():\n",
    "        restored = []\n",
    "        for canon_nl, orig_nl, e in bucket:\n",
    "            canon = nx.node_link_graph(canon_nl)\n",
    "            orig  = nx.node_link_graph(orig_nl)\n",
    "            restored.append((canon, orig, e))\n",
    "        registry[h] = restored\n",
    "    return registry\n",
    "\n",
    "\n",
    "def registry_size(reg):\n",
    "    #reg = load_registry(pkl_file)\n",
    "    length = sum(len(b) for b in reg.values())\n",
    "    print(f\"Registry length: {length}\")\n",
    "    return length\n",
    "\n",
    "def fast_registry_size(pkl_file):\n",
    "    \"\"\"\n",
    "    Quick-check the contents of a saved registry file.\n",
    "\n",
    "    Prints:\n",
    "      • total triples  (canon, orig, e)\n",
    "      • distinct hash buckets\n",
    "    Returns a tuple (n_hashes, n_items) in case you want to use it programmatically.\n",
    "    \"\"\"\n",
    "    import pickle, pathlib, os\n",
    "\n",
    "    pkl_file = os.fspath(pkl_file)          # accept Path or str\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        saved = pickle.load(f)\n",
    "\n",
    "    n_hashes = len(saved)                   # one key per bucket\n",
    "    n_items  = sum(len(bucket) for bucket in saved.values())\n",
    "\n",
    "    print(f\"{n_items:,} items across {n_hashes:,} hash buckets\")\n",
    "    return n_hashes, n_items\n",
    "\n",
    "\n",
    "def draw_pair(canon, orig, h, e, seed=42):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(7, 3.5))\n",
    "    for ax, g, ttl in zip(axes, (canon, orig), (\"canonical\", \"original\")):\n",
    "        pos = nx.spring_layout(g, seed=seed)\n",
    "        nx.draw(g, pos, ax=ax, with_labels=True, node_size=100, font_size=7)\n",
    "        ax.set_title(ttl)\n",
    "    fig.suptitle(f\"hash={h}   energy={e:.3f}\", fontsize=10)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def top_lowest_energy_plots(reg, n=20, pause=0):\n",
    "    \"\"\"\n",
    "    Show a ranked table and inline plots for the `n` lowest-energy entries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pkl_file : str or Path\n",
    "        Pickled registry file.\n",
    "    n : int, optional (default 20)\n",
    "        Number of graph pairs to display.\n",
    "    pause : float, optional (default 0)\n",
    "        Seconds to wait between figures.\n",
    "    \"\"\"\n",
    "    #reg = load_registry(pkl_file)\n",
    "    flat = [\n",
    "        (e, h, canon, orig)\n",
    "        for h, bucket in reg.items()\n",
    "        for canon, orig, e in bucket\n",
    "    ]\n",
    "    flat.sort(key=lambda t: t[0])\n",
    "\n",
    "    print(f\"\\nTop-{n} by lowest energy:\\n\")\n",
    "    print(f\"{'rank':>4} │ {'energy':>10} │ {'hash':<16} │ nodes\")\n",
    "    print(\"────┼────────────┼────────────────┼──────\")\n",
    "    for i, (e, h, canon, orig) in enumerate(flat[:n], 1):\n",
    "        print(f\"{i:>4} │ {e:10.4f} │ {h:<16} │ {orig.number_of_nodes():>5}\")\n",
    "        draw_pair(canon, orig, h, e)\n",
    "        if pause:\n",
    "            time.sleep(pause)\n",
    "\n",
    "def iso_pairs_lowest(reg, top=20):\n",
    "    \"\"\"\n",
    "    Among the `top` lowest-energy *canonical* graphs, print every pair that\n",
    "    is isomorphic.  Returns a list of (i, j) index pairs.\n",
    "    \"\"\"\n",
    "    #reg = load_registry(pkl_file)\n",
    "\n",
    "    # flatten and keep canonical graph\n",
    "    flat = [(e, h, canon)\n",
    "            for h, bucket in reg.items()\n",
    "            for canon, _, e in bucket]\n",
    "    flat.sort(key=lambda t: t[0])\n",
    "    flat = flat[:top]                       # lowest-energy slice\n",
    "\n",
    "    duplicates = []\n",
    "    for (i, (e_i, h_i, g_i)), (j, (e_j, h_j, g_j)) in itertools.combinations(enumerate(flat), 2):\n",
    "        if nx.is_isomorphic(g_i, g_j):\n",
    "            duplicates.append((i, j))\n",
    "            print(f\"Duplicate pair: idx {i} ↔ {j}   \"\n",
    "                  f\"energies {e_i:.4f} / {e_j:.4f}   \"\n",
    "                  f\"hashes {h_i} / {h_j}\")\n",
    "\n",
    "    if not duplicates:\n",
    "        print(f\"No canonical duplicates among the lowest {top} energies.\")\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "def remove_redundant_edges(g):\n",
    "    \"\"\"Return a copy of `g` with every non-essential edge pruned.\"\"\"\n",
    "    g = g.copy()\n",
    "    tt_ref = calculate_truth_table_v2(g)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for u, v in list(g.edges()):\n",
    "            g_tmp = g.copy()\n",
    "            g_tmp.remove_edge(u, v)\n",
    "            if calculate_truth_table_v2(g_tmp) == tt_ref:\n",
    "                g.remove_edge(u, v)\n",
    "                changed = True\n",
    "    return g\n",
    "\n",
    "def unique_by_isomorphism(graphs):\n",
    "    uniq = []\n",
    "    for g in graphs:\n",
    "        if not any(nx.is_isomorphic(g, h) for h in uniq):\n",
    "            uniq.append(g)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "def plot_graphs(graphs, cols=3, seed=42):\n",
    "    rows = (len(graphs) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    axes = axes.ravel()\n",
    "    for ax, g in zip(axes, graphs):\n",
    "        pos = nx.spring_layout(g, seed=seed)\n",
    "        nx.draw(g, pos, ax=ax, with_labels=True, node_size=100, font_size=7)\n",
    "        ax.set_title(f\"nodes={g.number_of_nodes()}  edges={g.number_of_edges()}\")\n",
    "    for ax in axes[len(graphs):]:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prune_and_plot_optimal(reg):\n",
    "    \"\"\"\n",
    "    • keep every graph at the absolute minimum energy\n",
    "    • remove redundant edges in each\n",
    "    • drop isomorphic duplicates\n",
    "    • plot & print truth tables\n",
    "    • RETURN the list of unique pruned graphs\n",
    "    \"\"\"\n",
    "    #reg = load_registry(pkl_file)\n",
    "\n",
    "    # -- keep only the optimal energy ----------------------------------------\n",
    "    flat = [(e, canon) for bucket in reg.values() for canon, _, e in bucket]\n",
    "    best_energy = min(e for e, _ in flat)\n",
    "    best_graphs = [canon for e, canon in flat if e == best_energy]\n",
    "    print(f\"Optimal energy: {best_energy:.4f}   raw count: {len(best_graphs)}\")\n",
    "\n",
    "    # -- prune redundant edges -----------------------------------------------\n",
    "    pruned = [remove_redundant_edges(g) for g in best_graphs]\n",
    "\n",
    "    # -- drop isomorphic duplicates ------------------------------------------\n",
    "    unique = unique_by_isomorphism(pruned)\n",
    "    print(f\"{len(unique)} unique graph(s) remain after pruning + iso check.\\n\")\n",
    "\n",
    "    # -- plot each graph and print its truth table ---------------------------\n",
    "    for idx, g in enumerate(unique, 1):\n",
    "        # plot\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        pos = nx.spring_layout(g, seed=42)\n",
    "        nx.draw(g, pos, with_labels=True, node_size=100, font_size=7)\n",
    "        plt.title(f\"Graph {idx}   nodes={g.number_of_nodes()}  edges={g.number_of_edges()}\")\n",
    "        plt.show()\n",
    "\n",
    "        # truth table\n",
    "        tt = calculate_truth_table_v2(g)\n",
    "        print(f\"Truth table for Graph {idx}:\")\n",
    "        for inputs, out in sorted(tt.items()):\n",
    "            print(f\"  {inputs}  →  {out[0]}\")\n",
    "        print()\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "def energies_from_log(txt_file):\n",
    "    pat = re.compile(r\"^\\s*(?:\\Selected)\\s+(.*_NIG_unoptimized\\.pkl)$\")\n",
    "    paths = [m.group(1).strip() for line in open(txt_file)\n",
    "             if (m := pat.match(line))]\n",
    "    if not paths:\n",
    "        print(\"No circuit paths found in the file.\"); return\n",
    "\n",
    "    rows = []\n",
    "    for fp in tqdm(paths, desc=\"Processing circuits\", unit=\"circuit\"):\n",
    "        G = load_graph_pickle(fp)\n",
    "        E_orig, _ = energy_score(G, check_implicit_OR_existence_v3)\n",
    "\n",
    "        canon = _apply_implicit_or(G)\n",
    "        E_canon, _ = energy_score(canon, check_implicit_OR_existence_v3)\n",
    "\n",
    "        hex_id = Path(fp).name.split(\"_\")[0]\n",
    "        rows.append((hex_id, E_orig, E_canon))\n",
    "\n",
    "    # pretty print\n",
    "    print(f\"\\nEnergy summary ({len(rows)} circuits):\\n\")\n",
    "    print(f\"{'hex':<6} │ {'orig':>8} │ {'canon':>8}\")\n",
    "    print(\"───────┼──────────┼──────────\")\n",
    "    for h, e1, e2 in rows:\n",
    "        print(f\"{h:<6} │ {e1:8.3f} │ {e2:8.3f}\")\n",
    "\n",
    "def load_graph_pickle(filename):\n",
    "    num_nodes, edges, node_attrs = pickle.load(open(filename, \"rb\"))\n",
    "    G = nx.DiGraph()\n",
    "    for n, attr in node_attrs.items():\n",
    "        G.add_node(n, type=attr) if attr is not None else G.add_node(n)\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "def truth_table_signature(tt):\n",
    "    \"\"\"\n",
    "    Convert the dict {inputs → outputs} returned by calculate_truth_table_v2\n",
    "    into a single immutable tuple that can be used as a dict key.\n",
    "    For multi-output circuits we simply concatenate the bits.\n",
    "    \"\"\"\n",
    "    flat = []\n",
    "    for inp in sorted(tt):                 # lexicographic input order\n",
    "        # 'outputs' is a tuple even for single-output circuits\n",
    "        flat.extend(tt[inp])\n",
    "    return tuple(flat)                     # e.g. (0,1,1,0,1,0,0,1)\n",
    "\n",
    "# ── 2.  regroup the existing registry by truth table ──────────────────────────\n",
    "def split_registry_by_truth_table(registry):\n",
    "    \"\"\"\n",
    "    Return a dict  {signature → list of (hash_id, canon, orig, energy)}.\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for h, bucket in registry.items():\n",
    "        for canon, orig, e in bucket:\n",
    "            sig = truth_table_signature(calculate_truth_table_v2(canon))\n",
    "            buckets[sig].append((h, canon, orig, e))\n",
    "\n",
    "    print(f\"Found {len(buckets)} distinct truth tables.\")\n",
    "    return buckets\n",
    "\n",
    "\n",
    "\n",
    "def split_registry_by_truth_table_with_inputs_permutations(registry):\n",
    "  \n",
    "    \n",
    "    print(\"Calculating allowed boolean functions\")\n",
    "    allowed_signatures = {truth_table_signature(calculate_truth_table_v2(canon)) for bucket in registry.values() for canon, _, _ in bucket}    \n",
    "    print(f\"Done calculating allowed boolean functions. Found {len(allowed_signatures)}\")         \n",
    "    \n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for h, bucket in tqdm(registry.items(), total=len(registry), desc=\"Hash groups\", unit=\"group\"):\n",
    "        for canon, orig, energy in bucket:\n",
    "\n",
    "            # primary inputs (sources) — same criterion as in _permute_and_match\n",
    "            inputs = [n for n in canon if canon.in_degree(n) == 0]\n",
    "\n",
    "            permutations = itertools.permutations(inputs)\n",
    "\n",
    "            for perm in permutations:\n",
    "                \n",
    "                mapping = dict(zip(inputs, perm))\n",
    "                canon_permuted_inputs = nx.relabel_nodes(canon, mapping, copy=True)\n",
    "\n",
    "                sig = truth_table_signature(calculate_truth_table_v2(canon_permuted_inputs))\n",
    "                \n",
    "                if sig in allowed_signatures:\n",
    "                    buckets[sig].append((h, canon_permuted_inputs, orig, energy))\n",
    "\n",
    "    print(f\"Found {len(buckets)} distinct truth tables (after input permutations).\")\n",
    "    return buckets\n",
    "\n",
    "\n",
    "\n",
    "def run_analysis_per_bucket(registry, permute_inputs = True):\n",
    "    \"\"\"\n",
    "    For every distinct truth table:\n",
    "        • build a mini-registry with the same shape as the original one\n",
    "        • call prune_and_plot_optimal, check_vs_motif_bank, …\n",
    "    \"\"\"\n",
    "    \n",
    "    if permute_inputs:\n",
    "        buckets = split_registry_by_truth_table_with_inputs_permutations(registry)\n",
    "    else:\n",
    "        buckets = split_registry_by_truth_table(registry)\n",
    "    \n",
    "    for idx, (sig, entries) in enumerate(buckets.items(), 1):\n",
    "        print(\"\\n\" + \"═\"*60)\n",
    "        print(f\"Truth table {idx}/{len(buckets)}  –  {len(entries)} circuit(s)\")\n",
    "        print(\"Signature:\", sig)    \n",
    "        print(\"Hex ID   :\", hex_from_signature(sig))\n",
    "        print(\"═\"*60)\n",
    "\n",
    "        # ---- rebuild a “registry-shaped” dict so your helpers work unchanged\n",
    "        mini = defaultdict(list)\n",
    "        for h, canon, orig, e in entries:\n",
    "            mini[h].append((canon, orig, e))\n",
    "\n",
    "        # ---- now reuse the tools you already wrote --------------------------\n",
    "        unique_graphs = prune_and_plot_optimal(mini)\n",
    "        check_vs_motif_bank(unique_graphs)\n",
    "        \n",
    "def plot_bucket_by_hex(\n",
    "    registry,\n",
    "    hex_id: str,\n",
    "    *,\n",
    "    permute_inputs=True,\n",
    "    show=True,\n",
    "):\n",
    "    # normalise input (\"3c96\", \"0X3C96\", … → \"3c96\")\n",
    "    hex_id = hex_id.lower().lstrip(\"0x\")\n",
    "\n",
    "    # 1) build buckets -------------------------------------------------------\n",
    "    buckets = (split_registry_by_truth_table_with_inputs_permutations(registry)\n",
    "               if permute_inputs else\n",
    "               split_registry_by_truth_table(registry))\n",
    "\n",
    "    # 2) locate the signature that matches the requested hex ----------------\n",
    "    target_sig = None\n",
    "    for sig in buckets:\n",
    "        if hex_from_signature(sig).lower().lstrip(\"0x\") == hex_id:\n",
    "            target_sig = sig\n",
    "            break\n",
    "\n",
    "    if target_sig is None:\n",
    "        raise ValueError(f\"Hex ID {hex_id} not found in registry.\")\n",
    "\n",
    "    entries = buckets[target_sig]\n",
    "    print(\"\\n\" + \"═\" * 60)\n",
    "    print(f\"Hex ID   : 0x{hex_id.upper()}\")\n",
    "    print(f\"Signature: {target_sig}\")\n",
    "    print(f\"Circuits : {len(entries)}\")\n",
    "    print(\"═\" * 60)\n",
    "\n",
    "    # 3) rebuild mini-registry ----------------------------------------------\n",
    "    mini = defaultdict(list)\n",
    "    for entry in entries:\n",
    "        h, canon, orig, e = entry[:4]\n",
    "        mini[h].append((canon, orig, e))\n",
    "\n",
    "    # -------- run your analysis helper ------------------------------------\n",
    "    unique_graphs = prune_and_plot_optimal(mini)   # <- no return_fig kwarg\n",
    "    check_vs_motif_bank(unique_graphs)\n",
    "\n",
    "'''     \n",
    "def _apply_implicit_or(G, fanin_size: int = 2):\n",
    "    G_copy = G.copy()\n",
    "    output_nodes = [n for n in G_copy if G_copy.out_degree(n) == 0]\n",
    "    if not output_nodes:\n",
    "        return G_copy\n",
    "    output_node = output_nodes[0]\n",
    "    results_check_implicit_OR_existence = check_implicit_OR_existence_v3(G_copy, output_node, fanin_size)\n",
    "    best_node_reduction_found, best_node_reduction_found_key = 0, None\n",
    "    for key, value in results_check_implicit_OR_existence.items():\n",
    "        if value[\"is_there_an_implicit_OR\"] and value[\"number_of_nodes_available_for_removal\"] > best_node_reduction_found:\n",
    "            best_node_reduction_found_key, best_node_reduction_found = key, value[\"number_of_nodes_available_for_removal\"]\n",
    "    if best_node_reduction_found_key is None:\n",
    "        return G_copy\n",
    "    cut = results_check_implicit_OR_existence[best_node_reduction_found_key][\"cut\"]\n",
    "    cone = results_check_implicit_OR_existence[best_node_reduction_found_key][\"cone\"]\n",
    "    return add_implicit_OR_to_dag_v2(G_copy, output_node, cut, cone)\n",
    "'''\n",
    "\n",
    "def tag_value(p: Path) -> int:\n",
    "    \"\"\"Return the numeric tag, e.g. '4_800' -> 4800.\"\"\"\n",
    "    m = rx.fullmatch(p.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Not a registry file: {p}\")\n",
    "    return int(m.group(1).replace(\"_\", \"\"))     # drop underscores, then int()\n",
    "\n",
    "\n",
    "def load_selected_paths(txt_file):\n",
    "    \"\"\"\n",
    "    Parse *selected_graphs.txt* and return a list of absolute Path objects.\n",
    "\n",
    "    Handles lines such as\n",
    "        Selected /path/with spaces/0x66_NIG_unoptimized.pkl\n",
    "        /full/or/relative/0x23_NIG_unoptimized.pkl\n",
    "\n",
    "    • If the line starts with the literal word “Selected”, the remainder of\n",
    "      the line is taken verbatim as the path.\n",
    "    • Otherwise the whole line is treated as the path.\n",
    "    • Relative paths (very unlikely in your case) are resolved relative to\n",
    "      the folder that contains *selected_graphs.txt*.\n",
    "    \"\"\"\n",
    "    txt_file = Path(txt_file)\n",
    "    base_dir = txt_file.parent\n",
    "    paths    = []\n",
    "\n",
    "    for line in txt_file.read_text().splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue                                # skip empty lines\n",
    "\n",
    "        path_str = line.split(maxsplit=1)[1] if line.startswith(\"Selected \") else line\n",
    "        p = Path(path_str)\n",
    "\n",
    "        if not p.is_absolute():\n",
    "            p = base_dir / p                       # anchor relative paths\n",
    "\n",
    "        p = p.resolve()\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"{p} (from {txt_file}) does not exist\")\n",
    "        paths.append(p)\n",
    "\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"No pickle paths found in {txt_file}\")\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def bucket_unoptimised(txt_file):\n",
    "    \"\"\"\n",
    "    Return {signature → list of (file_path, G_orig, E_orig)} for the unoptimised set.\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    paths   = load_selected_paths(txt_file)\n",
    "\n",
    "    for fp in tqdm(paths, desc=\"Loading unoptimised graphs\", unit=\"graph\"):\n",
    "        G_orig  = load_graph_pickle(fp)                      # Path object works fine\n",
    "        E_orig, _ = energy_score(G_orig, check_implicit_OR_existence_v3)\n",
    "\n",
    "        canon  = _apply_implicit_or(G_orig)                  # stabilise truth table\n",
    "        sig    = truth_table_signature(calculate_truth_table_v2(canon))\n",
    "\n",
    "        buckets[sig].append((fp, G_orig, E_orig))\n",
    "    return buckets\n",
    "\n",
    "def compare_registry_vs_unoptimised(registry, selected_txt, permute_inputs = True):\n",
    "    \"\"\"\n",
    "    For every truth-table that exists in *either* source:\n",
    "        • print the best energy in the registry   (optimised / canon)\n",
    "        • print the best energy in selected_txt   (unoptimised)\n",
    "        • plot the two winning graphs\n",
    "    \"\"\"\n",
    "    if permute_inputs:\n",
    "        can_buckets = split_registry_by_truth_table_with_inputs_permutations(registry)\n",
    "    else:\n",
    "        can_buckets = split_registry_by_truth_table(registry)\n",
    "        \n",
    "    unopt_buckets = bucket_unoptimised(selected_txt)\n",
    "\n",
    "    header = f\"{'TT#':>4} │ {'E canon':>9} │ {'hash':<12} │ \"\\\n",
    "             f\"{'E unopt':>9} │ {'file':<28} │ ΔE\"\n",
    "    sep = \"────┼──────────┼──────────────┼──────────┼────────────────────────────┼────────\"\n",
    "    print(\"\\n\"+header);  print(sep)\n",
    "\n",
    "    # iterate over union of truth-tables so nothing is missed\n",
    "    for idx, sig in enumerate(sorted(set(can_buckets) | set(unopt_buckets)), 1):\n",
    "\n",
    "        # -- best canonical ---------------------------------------------------\n",
    "        if sig in can_buckets:\n",
    "            h_c, g_canon, _, e_canon = min(can_buckets[sig], key=lambda t: t[3])\n",
    "        else:                                      # TT not in registry\n",
    "            h_c, g_canon, e_canon = \"—\", None, float(\"nan\")\n",
    "\n",
    "        # -- best unoptimised -------------------------------------------------\n",
    "        if sig in unopt_buckets:\n",
    "            fp_u, g_unopt, e_unopt = min(unopt_buckets[sig], key=lambda t: t[2])\n",
    "        else:                                      # TT not in selected list\n",
    "            fp_u, g_unopt, e_unopt = \"—\", None, float(\"nan\")\n",
    "\n",
    "        dE = (e_unopt - e_canon) if (not (e_canon!=e_canon) and not (e_unopt!=e_unopt)) else float(\"nan\")\n",
    "        print(f\"{idx:>4} │ {e_canon:9.4f} │ {h_c:<12} │ \"\n",
    "              f\"{e_unopt:9.4f} │ {Path(fp_u).name:<28} │ {dE:+.4f}\")\n",
    "\n",
    "        # -- plot the graphs so you can eyeball the difference ----------------\n",
    "        if g_canon and g_unopt:\n",
    "            g_unopt_can = _apply_implicit_or(g_unopt)       # <<< NEW line\n",
    "\n",
    "            plt.figure(figsize=(7, 3.5))\n",
    "            axes = plt.subplots(1, 2)[1]\n",
    "            for ax, g, ttl in zip(\n",
    "                    axes,\n",
    "                    (g_canon, g_unopt_can),                 # <<< use canonical here\n",
    "                    (\"optimised canonical\", \"unoptimised canonical\")):\n",
    "                pos = nx.spring_layout(g, seed=42)\n",
    "                nx.draw(g, pos, ax=ax,\n",
    "                        node_size=140, font_size=7, with_labels=True)\n",
    "                ax.set_title(ttl)\n",
    "\n",
    "            plt.suptitle(f\"Truth table {idx}   ΔE = {dE:+.4f}\", fontsize=10)\n",
    "            plt.tight_layout(); plt.show()    \n",
    "\n",
    "\n",
    "def hex_from_signature(sig):\n",
    "    \"\"\"\n",
    "    Convert a truth-table signature tuple, e.g. (0,1,1,0,0,1,1,0),\n",
    "    into its hexadecimal representation.\n",
    "\n",
    "    • The first element of `sig` is treated as the most-significant bit,\n",
    "      matching the convention in your NIG file names.\n",
    "    • Pads with leading zeroes so that 4 signature bits → 1 hex digit.\n",
    "    \"\"\"\n",
    "    val = 0\n",
    "    for bit in sig:\n",
    "        val = (val << 1) | bit\n",
    "    width = max(1, len(sig) // 4)              # 8 bits → 2 hex digits, etc.\n",
    "    return f\"0x{val:0{width}X}\"                # uppercase hex\n",
    "\n",
    "def bucket_unoptimised(txt_file):\n",
    "    buckets = defaultdict(list)\n",
    "    paths   = load_selected_paths(txt_file)\n",
    "\n",
    "    for fp in tqdm(paths, desc=\"Loading unoptimised graphs\", unit=\"graph\"):\n",
    "        G_orig   = load_graph_pickle(fp)\n",
    "        E_orig, _ = energy_score(G_orig, check_implicit_OR_existence_v3)\n",
    "\n",
    "        canon    = _apply_implicit_or(G_orig)\n",
    "        sig      = truth_table_signature(calculate_truth_table_v2(canon))\n",
    "\n",
    "        buckets[sig].append((fp, G_orig, E_orig))\n",
    "    return buckets\n",
    "\n",
    "def export_energies_optimized_versus_unoptimized(registry, selected_txt, csv_path=None, skip_incomplete=True, permute_inputs = True):\n",
    "    \"\"\"\n",
    "    CSV columns:\n",
    "        truth_table_hex, E_unoptimised, E_optimised\n",
    "    \"\"\"\n",
    "    if (permute_inputs):\n",
    "        can_buckets   = split_registry_by_truth_table_with_inputs_permutations(registry)\n",
    "    else:\n",
    "        can_buckets   = split_registry_by_truth_table(registry)\n",
    "    \n",
    "    unopt_buckets = bucket_unoptimised(selected_txt)\n",
    "\n",
    "    records = []\n",
    "    for sig in sorted(set(can_buckets) | set(unopt_buckets)):\n",
    "\n",
    "        hex_tt = hex_from_signature(sig)               \n",
    "\n",
    "        # best optimised\n",
    "        if sig in can_buckets:\n",
    "            e_c = min(can_buckets[sig], key=lambda t: t[3])[3]\n",
    "        else:\n",
    "            e_c = None\n",
    "\n",
    "        # best unoptimised\n",
    "        if sig in unopt_buckets:\n",
    "            _, _, e_u = min(unopt_buckets[sig], key=lambda t: t[2])\n",
    "        else:\n",
    "            e_u = None\n",
    "\n",
    "        if skip_incomplete and (e_c is None or e_u is None):\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"truth_table_hex\": hex_tt,\n",
    "            \"E_unoptimised\" : e_u,\n",
    "            \"E_optimised\"   : e_c\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # choose output path\n",
    "    if csv_path is None:\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_path = Path(selected_txt).with_name(f\"energy_comparison_{ts}.csv\")\n",
    "    else:\n",
    "        csv_path = Path(csv_path)\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {csv_path}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_bank_energy(graph, *, debug=False):\n",
    "    \"\"\"\n",
    "    Energy of the best motif in UNIQUE_GRAPHS that shares the exact\n",
    "    truth-table key with `graph`, using NO permutations.\n",
    "\n",
    "    The key is computed with `_compute_truth_key`, which returns a tuple\n",
    "    (n_inputs, int_key) – identical to the keys stored in TTABLE_TO_ACTIONS.\n",
    "    \"\"\"\n",
    "    key   = _compute_truth_key(graph)        \n",
    "    idxs  = TTABLE_TO_ACTIONS.get(key, [])\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] key = {key}  → mapped indices = {idxs}\")\n",
    "\n",
    "    if not idxs:\n",
    "        return None                          # bank has no entry\n",
    "\n",
    "    return min(\n",
    "        energy_score(UNIQUE_GRAPHS[j], check_implicit_OR_existence_v3)[0]\n",
    "        for j in idxs\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def export_energy_as_compared_to_action_motifs(\n",
    "    registry,\n",
    "    *,\n",
    "    csv_path=\"energy_vs_bank.csv\",\n",
    "    skip_incomplete=True,\n",
    "    debug=False,\n",
    "    permute_inputs=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each truth-table bucket in `registry`, write:\n",
    "\n",
    "        truth_table_hex, E_candidates, E_bank\n",
    "    \"\"\"\n",
    "        \n",
    "    if (permute_inputs):\n",
    "        buckets   = split_registry_by_truth_table_with_inputs_permutations(registry)\n",
    "    else:\n",
    "        buckets = split_registry_by_truth_table(registry)\n",
    "    \n",
    "    \n",
    "    print(f\"Found {len(buckets)} distinct truth tables.\")\n",
    "\n",
    "    rows = []\n",
    "    for designs in buckets.values():\n",
    "        g0       = designs[0][1]                # any graph in the bucket\n",
    "        sig  = truth_table_signature(calculate_truth_table_v2(g0))\n",
    "        hex_key = hex_from_signature(sig)            \n",
    "        #key      = _compute_truth_key(g0)       # tuple (n_inputs, int_key)\n",
    "        #hex_key  = hex(key[1])                  # just the integer part\n",
    "\n",
    "        best_cand_E = min(\n",
    "            energy_score(g[1], check_implicit_OR_existence_v3)[0]\n",
    "            for g in designs\n",
    "        )\n",
    "        best_bank_E = lowest_bank_energy(g0, debug=debug)\n",
    "\n",
    "        print(f\"  key {hex_key} – bank energy: {best_bank_E}\")\n",
    "\n",
    "        if skip_incomplete and best_bank_E is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"truth_table_hex\": hex_key,\n",
    "            \"E_candidates\":    best_cand_E,\n",
    "            \"E_bank\":          best_bank_E,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    Path(csv_path).with_suffix(\".csv\").write_text(df.to_csv(index=False))\n",
    "    print(f\"Saved {len(df)} rows to {csv_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def6de0-70ec-4729-9c15-f9ff87deeb37",
   "metadata": {},
   "source": [
    "Point to the latest shared registry in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e0a9b-975e-4cac-9f7f-a7d7235e31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx = re.compile(r\"shared_registry_([\\d_]+)\\.pkl$\")\n",
    "\n",
    "try:\n",
    "    latest = max(\n",
    "        (p for p in run_dir.glob(\"shared_registry_*.pkl\")),\n",
    "        key=tag_value\n",
    "    )\n",
    "    print(\"Latest snapshot:\", latest, \"steps =\", tag_value(latest))\n",
    "    pkl_path = latest\n",
    "except ValueError:\n",
    "    print(\"No shared_registry_*.pkl found in\", run_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2daae1-aff6-4285-ae1a-5b7e56fa7ca2",
   "metadata": {},
   "source": [
    "Calculate the registry size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_registry_size(pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5ec97",
   "metadata": {},
   "source": [
    "Load the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d144420-76d5-488f-a850-d8dd4be0687d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "registry = load_registry(pkl_path)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c4f78",
   "metadata": {},
   "source": [
    "Testing speed of operations on registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing, math\n",
    "\n",
    "manager   = multiprocessing.Manager()\n",
    "registry_across_workers = manager.dict()          \n",
    "multiprocessing_lock  = manager.Lock()\n",
    "best_energy_across_workers  = manager.Value('d', math.inf)  \n",
    "\n",
    "reg_path = Path(pkl_path).expanduser()\n",
    "print(f\"Loading registry at {reg_path}\")\n",
    "\n",
    "with reg_path.open(\"rb\") as f:\n",
    "    reg = pickle.load(f)       \n",
    "\n",
    "    for h, bucket in reg.items():\n",
    "        registry_across_workers[h] = [\n",
    "            (nx.node_link_graph(canon_nl),\n",
    "            nx.node_link_graph(orig_nl),\n",
    "            e)\n",
    "            for canon_nl, orig_nl, e in bucket\n",
    "        ]\n",
    "        for _c, _o, e in registry_across_workers[h]:\n",
    "            if e < best_energy_across_workers.value:\n",
    "                best_energy_across_workers.value = e\n",
    "\n",
    "\n",
    "print(f\"Loaded registry with {len(registry_across_workers)} hash buckets\"\n",
    "    f\"best Energy = {best_energy_across_workers.value:.3f}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(registry_across_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4df48ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_bucket = list(registry_across_workers.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78dded43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<networkx.classes.digraph.DiGraph at 0x7f37a3562be0>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f37a355dd60>,\n",
       "  50)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ee24e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_bucket_cannon_design = first_bucket[0][0]\n",
    "first_bucket_original_design = first_bucket[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a26a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61ba1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "canon = _apply_implicit_or(first_bucket_original_design)\n",
    "h     = _compute_hash(canon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1666a9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_isomorphic(canon, first_bucket_cannon_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd3ae53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'69c6ea5a6682880170d8f612ccda2c50'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f60bd604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<networkx.classes.digraph.DiGraph at 0x7f36897fe7f0>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f36897feca0>,\n",
       "  50)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = registry_across_workers.setdefault(h, [])\n",
    "bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67b372c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<networkx.classes.digraph.DiGraph at 0x7f36897fe7f0>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f36897feca0>,\n",
       "  50)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0ac4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(nx.is_isomorphic(canon, bucket[0][0]))\n",
    "print(nx.is_isomorphic(first_bucket_cannon_design, bucket[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df4678d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 'test', 1), ('test', 'test', 2)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 'nonexistenhash'\n",
    "bucket = registry_across_workers.setdefault(h, [])\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f53bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.append(('test', 'test', 3))\n",
    "registry_across_workers[h] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f3892cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 'test', 1), ('test', 'test', 2), ('test', 'test', 3)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 'nonexistenhash'\n",
    "bucket = registry_across_workers.setdefault(h, [])\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_singletons(registry):\n",
    "    for k in registry.keys():         # keys() is cheap (no big payloads)\n",
    "        if len(registry[k]) != 1:     # pulls that one list; exits early if >1\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(\"Only one item per bucket?\", only_singletons(registry_across_workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for k in registry_across_workers.keys():\n",
    "    total += len(registry_across_workers[k])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03379ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(registry_across_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae58a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = tuple(registry_across_workers.keys())\n",
    "h = keys[np.random.randint(len(keys))]\n",
    "bucket = registry_across_workers[h]\n",
    "canon, orig, e = bucket[np.random.randint(len(bucket))] \n",
    "current_solution = orig.copy()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e02120",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fa6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(registry_across_workers.keys())\n",
    "#k = int(self.np_random.integers(len(keys)))   # reproducible with your RNG\n",
    "#h = keys[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a228d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def analyze_buckets_iso_after_canon(\n",
    "    registry,\n",
    "    canon_fn,                   # e.g., _apply_implicit_or\n",
    "    check_attr_key=\"type\",      # set to None to skip attribute-aware check\n",
    "    sample=None,                # int: randomly sample this many multi-item buckets (for speed)\n",
    "    rng_seed=0,\n",
    "    max_examples=10\n",
    "):\n",
    "    \"\"\"\n",
    "    For each bucket with size > 1:\n",
    "      - Recompute canonical form: canon_fn(orig.copy())\n",
    "      - Check structural isomorphism among canonical graphs\n",
    "      - Optionally check attribute-aware isomorphism (node attribute == check_attr_key)\n",
    "\n",
    "    Returns summary dict and prints a brief report.\n",
    "    \"\"\"\n",
    "    # 1) Which buckets have >1?\n",
    "    gt1_keys = [k for k,b in registry.items() if len(b) > 1]\n",
    "    total_buckets = len(registry)\n",
    "    num_gt1 = len(gt1_keys)\n",
    "\n",
    "    # Optional subsample for speed\n",
    "    if sample is not None and sample < num_gt1:\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "        gt1_keys = list(rng.choice(gt1_keys, size=sample, replace=False))\n",
    "        sampled = True\n",
    "    else:\n",
    "        sampled = False\n",
    "\n",
    "    size_hist = Counter(len(registry[k]) for k in gt1_keys)\n",
    "\n",
    "    all_iso_structural = 0\n",
    "    all_iso_with_attr = 0\n",
    "    not_iso_examples = []\n",
    "\n",
    "    def degsig(g):\n",
    "        if g.is_directed():\n",
    "            indeg = sorted(d for _, d in g.in_degree())\n",
    "            outdeg = sorted(d for _, d in g.out_degree())\n",
    "            return (tuple(indeg), tuple(outdeg))\n",
    "        else:\n",
    "            deg = sorted(d for _, d in g.degree())\n",
    "            return (tuple(deg), ())\n",
    "\n",
    "    node_match = None\n",
    "    if check_attr_key is not None:\n",
    "        node_match = lambda a, b: a.get(check_attr_key, None) == b.get(check_attr_key, None)\n",
    "\n",
    "    for h in gt1_keys:\n",
    "        bucket = registry[h]\n",
    "        # Recompute canonical graphs from the originals\n",
    "        canons = [canon_fn(orig.copy()) for _, orig, _ in bucket]\n",
    "        g0 = canons[0]\n",
    "\n",
    "        # fast prechecks\n",
    "        same_counts = all(\n",
    "            (g.number_of_nodes() == g0.number_of_nodes() and\n",
    "             g.number_of_edges() == g0.number_of_edges())\n",
    "            for g in canons[1:]\n",
    "        )\n",
    "        if not same_counts:\n",
    "            if len(not_iso_examples) < max_examples:\n",
    "                not_iso_examples.append((h, len(bucket), \"node/edge mismatch after canon\"))\n",
    "            continue\n",
    "\n",
    "        sig0 = degsig(g0)\n",
    "        if any(degsig(g) != sig0 for g in canons[1:]):\n",
    "            if len(not_iso_examples) < max_examples:\n",
    "                not_iso_examples.append((h, len(bucket), \"degree signature mismatch after canon\"))\n",
    "            continue\n",
    "\n",
    "        # structural iso on canonical graphs\n",
    "        structural_ok = all(nx.is_isomorphic(g0, g) for g in canons[1:])\n",
    "        if not structural_ok:\n",
    "            if len(not_iso_examples) < max_examples:\n",
    "                not_iso_examples.append((h, len(bucket), \"failed structural isomorphism after canon\"))\n",
    "            continue\n",
    "\n",
    "        all_iso_structural += 1\n",
    "\n",
    "        # attribute-aware iso (optional)\n",
    "        if node_match is not None:\n",
    "            with_attr_ok = all(nx.is_isomorphic(g0, g, node_match=node_match) for g in canons[1:])\n",
    "            if with_attr_ok:\n",
    "                all_iso_with_attr += 1\n",
    "            else:\n",
    "                if len(not_iso_examples) < max_examples:\n",
    "                    not_iso_examples.append((h, len(bucket), f\"failed attr isomorphism on '{check_attr_key}' after canon\"))\n",
    "\n",
    "    denom = len(gt1_keys) if gt1_keys else 0\n",
    "    summary = {\n",
    "        \"total_buckets\": total_buckets,\n",
    "        \"buckets_with_size_gt1\": num_gt1,\n",
    "        \"analyzed_buckets\": denom,\n",
    "        \"sampled\": sampled,\n",
    "        \"size_hist_gt1\": dict(sorted(size_hist.items())),\n",
    "        \"all_iso_structural_count\": all_iso_structural,\n",
    "        \"all_iso_structural_pct\": (100.0 * all_iso_structural / denom) if denom else 0.0,\n",
    "        \"all_iso_with_attr_count\": all_iso_with_attr if check_attr_key is not None else None,\n",
    "        \"all_iso_with_attr_pct\": (100.0 * all_iso_with_attr / denom) if (denom and check_attr_key is not None) else None,\n",
    "        \"not_iso_examples\": not_iso_examples,\n",
    "    }\n",
    "\n",
    "    print(f\"Total buckets: {total_buckets}\")\n",
    "    print(f\"Buckets with >1 graph: {num_gt1}\")\n",
    "    print(f\"Analyzed buckets: {denom}\" + (\" (sampled)\" if sampled else \"\"))\n",
    "    if denom:\n",
    "        print(\"Bucket-size histogram (sizes >1):\", dict(sorted(size_hist.items())))\n",
    "        print(f\"All-iso after canon (structural): {all_iso_structural}/{denom} \"\n",
    "              f\"({summary['all_iso_structural_pct']:.2f}%)\")\n",
    "        if check_attr_key is not None:\n",
    "            print(f\"All-iso after canon (node_match '{check_attr_key}'): \"\n",
    "                  f\"{all_iso_with_attr}/{denom} \"\n",
    "                  f\"({summary['all_iso_with_attr_pct']:.2f}%)\")\n",
    "        if not_iso_examples:\n",
    "            print(\"\\nExamples of non-isomorphic buckets (up to first \"\n",
    "                  f\"{len(not_iso_examples)}):\")\n",
    "            for key, sz, reason in not_iso_examples:\n",
    "                print(f\"  key={key!r}  size={sz}  reason={reason}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "summary = analyze_buckets_iso_after_canon(\n",
    "    registry_across_workers,                       # your loaded registry dict\n",
    "    canon_fn=_apply_implicit_or,    # <- your canon transform\n",
    "    check_attr_key=\"type\",          # or None\n",
    "    sample=None,                    # or e.g. 2000 to subsample for speed\n",
    "    rng_seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgd.environments.drl3env_loader5 import _apply_implicit_or, _compute_hash, _compute_truth_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf32bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "buket_test = registry_across_workers[\"42eb45f79ef21f4d75c9210008069fc3\"]\n",
    "cannon_graphs = []\n",
    "for cannon, original, energy in buket_test:\n",
    "    h = _compute_hash(cannon)\n",
    "    print(h)\n",
    "    cannon_graphs.append(cannon)\n",
    "\n",
    "isomorphic_pairs = []\n",
    "n = len(cannon_graphs)\n",
    "total = n * (n - 1) // 2\n",
    "print(f\"Checking {total} pairs...\")\n",
    "\n",
    "for (i, Gi), (j, Gj) in combinations(enumerate(cannon_graphs), 2):\n",
    "    iso = nx.is_isomorphic(Gi, Gj)\n",
    "    print(f\"({i}, {j}): {'isomorphic' if iso else 'not isomorphic'}\")\n",
    "    if iso:\n",
    "        isomorphic_pairs.append((i, j))\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Graphs: {n}\")\n",
    "print(f\"Pairs checked: {total}\")\n",
    "print(f\"Isomorphic pairs: {len(isomorphic_pairs)}\")\n",
    "print(isomorphic_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63865e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in cannon_graphs:\n",
    "    print(_compute_truth_key(g))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side(graphs, k=None, indices=None, seed=7, with_labels=True):\n",
    "    \"\"\"\n",
    "    Plot graphs side-by-side.\n",
    "    - Set k to plot the first k graphs.\n",
    "    - Or pass explicit indices=[i,j,k].\n",
    "    \"\"\"\n",
    "    if not graphs:\n",
    "        raise ValueError(\"No graphs provided.\")\n",
    "\n",
    "    if indices is not None:\n",
    "        sel = list(indices)\n",
    "    else:\n",
    "        k = len(graphs) if k is None else min(k, len(graphs))\n",
    "        sel = list(range(k))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(sel), figsize=(4*len(sel), 4))\n",
    "    if len(sel) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, i in zip(axes, sel):\n",
    "        G = graphs[i]\n",
    "        pos = nx.spring_layout(G, seed=seed)\n",
    "        nx.draw_networkx(G, pos=pos, ax=ax, with_labels=with_labels,\n",
    "                         node_size=400, font_size=8, arrows=G.is_directed())\n",
    "        ax.set_title(f\"G{i}: n={G.number_of_nodes()}, m={G.number_of_edges()}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Examples:\n",
    "# First 3:\n",
    "plot_side_by_side(cannon_graphs, k=3, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = \"432f\"\n",
    "bucket = registry_across_workers.setdefault(h, [])\n",
    "if not bucket:\n",
    "    print(\"No encontre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabde0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import weisfeiler_lehman_graph_hash \n",
    "for g in cannon_graphs:      \n",
    "    h = weisfeiler_lehman_graph_hash(g, node_attr=None, edge_attr=None, iterations=100, digest_size=16)\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "\n",
    "bi = np.random.randint(len(registry_across_workers))\n",
    "bucket = registry_across_workers[bi]\n",
    "item_idx = np.random.randint(len(bucket))\n",
    "canon, orig, e = bucket[item_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45de36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(registry_across_workers.keys())\n",
    "len(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc138596",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(len(b) for b in buckets)\n",
    "if total == 0:\n",
    "    raise ValueError(\"Registry is empty\")\n",
    "\n",
    "# pick an index in [0, total)\n",
    "r = np.random.randint(total)   # instead of int(np.random.integers(total))\n",
    "\n",
    "for b in buckets:\n",
    "    if r < len(b):\n",
    "        _, orig, _ = b[r]\n",
    "        current_solution = orig.copy()\n",
    "        break\n",
    "    r -= len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_items  = [item for bucket in registry_across_workers.values() for item in bucket]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool     = [orig for _, orig, _ in reg_items]\n",
    "energies = [e    for _, _,    e in reg_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "weights = [1.0 / (e ** p) for e in energies]\n",
    "\n",
    "# self.np_random draw proportional to the weights\n",
    "#self.current_solution = random.choices(pool, weights=weights, k=1)[0].copy()\n",
    "weights = np.asarray(weights, dtype=np.float64)\n",
    "prob    = weights / weights.sum()          # must sum to 1 for NumPy\n",
    "idx     = np.random.choice(len(pool), p=prob)\n",
    "current_solution = pool[idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_keys = [\"0x13CE, 0x4A32\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [n for n in current_solution if current_solution.in_degree(n) == 0]       \n",
    "perms = list(itertools.permutations(inputs))\n",
    "np.random.shuffle(perms)\n",
    "for perm in perms:                     # iterate without replacement\n",
    "    mapping  = dict(zip(inputs, perm))\n",
    "    g_perm   = nx.relabel_nodes(current_solution, mapping, copy=True)\n",
    "    \n",
    "    if _compute_truth_key(g_perm) in existing_keys: \n",
    "        current_solution = g_perm\n",
    "        break # stop at the first valid permutation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd868fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f606ab4",
   "metadata": {},
   "source": [
    "Calcualte the registry size after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb291ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_size(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1502e28",
   "metadata": {},
   "source": [
    "Specify file with database graph paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_txt = run_dir / \"selected_graphs.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5331465",
   "metadata": {},
   "source": [
    "Compare optimized and unoptimized graphs and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = run_dir/\"energies_optimized_versus_unoptimized.csv\"\n",
    "df = export_energies_optimized_versus_unoptimized(registry, selected_txt, file, permute_inputs = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf82ace",
   "metadata": {},
   "source": [
    "Compare optimized graphs versus action motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTIFS_PATH = \"/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/scripts/action_motifs.pkl\"\n",
    "with open(MOTIFS_PATH, \"rb\") as f:\n",
    "    action_motifs = pickle.load(f)\n",
    "\n",
    "UNIQUE_GRAPHS = action_motifs[\"graphs\"]       \n",
    "TTABLE_TO_ACTIONS = action_motifs[\"lookup\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = run_dir/\"energies_optimized_versus_action_motifs.csv\"\n",
    "df2 = export_energy_as_compared_to_action_motifs(registry, csv_path=file, debug = True, permute_inputs = False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e62d0",
   "metadata": {},
   "source": [
    "Other functions to expand on later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f83bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_by_truth_table = split_registry_by_truth_table_with_inputs_permutations(registry)\n",
    "print(len(buckets_by_truth_table))\n",
    "\n",
    "run_analysis_per_bucket(registry, permute_inputs = False)   \n",
    "\n",
    "hex_id = \"0xB3\"\n",
    "plot_bucket_by_hex(registry, hex_id = hex_id, permute_inputs=False, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd55c62",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
