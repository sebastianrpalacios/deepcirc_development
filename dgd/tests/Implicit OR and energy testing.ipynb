{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7629ea",
   "metadata": {},
   "source": [
    "# Testing DESIGNR's implicit OR and energy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from dgd.utils.utils5 import *\n",
    "from itertools import cycle\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "import networkx as nx\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "#import pygraphviz as pgv\n",
    "import json5 as json  # Import json5 instead of json\n",
    "import re\n",
    "import networkx as nx\n",
    "import numbers\n",
    "from typing import Dict, Hashable, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696f90c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23720dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signals_list_binary_3_input = [\n",
    "    {0: 0, 1: 0, 2: 0},  # First set of input signals\n",
    "    {0: 0, 1: 0, 2: 1},  # Second set of input signals\n",
    "    {0: 0, 1: 1, 2: 0},  # Third set of input signals\n",
    "    {0: 0, 1: 1, 2: 1},  # First set of input signals\n",
    "    {0: 1, 1: 0, 2: 0},  # Second set of input signals\n",
    "    {0: 1, 1: 0, 2: 1},  # Third set of input signals    \n",
    "    {0: 1, 1: 1, 2: 0},  # Second set of input signals\n",
    "    {0: 1, 1: 1, 2: 1},  # Third set of input signals  \n",
    "]   \n",
    "\n",
    "input_signals_list_binary_4_input = [\n",
    "    {0: 0, 1: 0, 2: 0, 3: 0},  \n",
    "    {0: 0, 1: 0, 2: 0, 3: 1},  \n",
    "    {0: 0, 1: 0, 2: 1, 3: 0}, \n",
    "    {0: 0, 1: 0, 2: 1, 3: 1}, \n",
    "    {0: 0, 1: 1, 2: 0, 3: 0},  \n",
    "    {0: 0, 1: 1, 2: 0, 3: 1},    \n",
    "    {0: 0, 1: 1, 2: 1, 3: 0},  \n",
    "    {0: 0, 1: 1, 2: 1, 3: 1}, \n",
    "    {0: 1, 1: 0, 2: 0, 3: 0},  \n",
    "    {0: 1, 1: 0, 2: 0, 3: 1},  \n",
    "    {0: 1, 1: 0, 2: 1, 3: 0}, \n",
    "    {0: 1, 1: 0, 2: 1, 3: 1}, \n",
    "    {0: 1, 1: 1, 2: 0, 3: 0},  \n",
    "    {0: 1, 1: 1, 2: 0, 3: 1},    \n",
    "    {0: 1, 1: 1, 2: 1, 3: 0},  \n",
    "    {0: 1, 1: 1, 2: 1, 3: 1},     \n",
    "]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_dags(folder_path):\n",
    "    graphs = {}\n",
    "    # Walk through each directory in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"_outputNetlist.json\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {full_path}\")  # Print the file being processed\n",
    "                data = load_json_file(full_path)\n",
    "                graph = create_dag_from_json(data)\n",
    "                # Use the directory name as the key for the graph\n",
    "                dir_name = os.path.basename(root)\n",
    "                graphs[dir_name] = graph\n",
    "    return graphs\n",
    "\n",
    "def load_json_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        file_content = file.read()\n",
    "        # Fix trailing commas in JSON objects and arrays\n",
    "        file_content = re.sub(r',(\\s*[\\]}])', r'\\1', file_content)\n",
    "        # Additional cleanup: Ensure no trailing commas at the end of the entire JSON content\n",
    "        file_content = re.sub(r',\\s*\\Z', '', file_content)\n",
    "        data = json.loads(file_content)\n",
    "    return data\n",
    "\n",
    "def create_dag_from_json(data):\n",
    "    G = nx.DiGraph()\n",
    "    # Add nodes to the graph\n",
    "    for node in data['nodes']:\n",
    "        G.add_node(node['name'], nodeType=node['nodeType'], deviceName=node['deviceName'])\n",
    "    # Add edges to the graph\n",
    "    for edge in data['edges']:\n",
    "        G.add_edge(edge['src'], edge['dst'], name=edge['name'])\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade79fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topology_plot_with_attrs(G, node_attr = None, edge_attr = None, seed = 42):\n",
    "\n",
    "    if node_attr is None:\n",
    "        first_node_attrs = next(iter(G.nodes(data=True)), (None, {}))[1]\n",
    "        node_attr = next(iter(first_node_attrs), None)\n",
    "\n",
    "    if edge_attr is None:\n",
    "        for _, _, d in G.edges(data=True):\n",
    "            if d:\n",
    "                edge_attr = next(iter(d))\n",
    "                break\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=seed)\n",
    "    plt.figure(figsize=(5, 5)); plt.axis(\"off\")\n",
    "\n",
    "    node_labels = {}\n",
    "    default_colour = \"lightblue\"          \n",
    "    node_colors   = default_colour\n",
    "\n",
    "    if node_attr is not None:\n",
    "        values = [G.nodes[n].get(node_attr) for n in G.nodes()]\n",
    "\n",
    "\n",
    "        if all(isinstance(v, (int, float)) for v in values if v is not None):\n",
    "            norm = mcolors.Normalize(vmin=min(values), vmax=max(values))\n",
    "            cmap = cm.get_cmap(\"viridis\")\n",
    "            node_colors = [cmap(norm(v)) for v in values]\n",
    "\n",
    "\n",
    "        else:\n",
    "            present_vals = {v for v in values if v is not None}\n",
    "\n",
    "            unique_vals  = sorted(present_vals, key=lambda x: str(x))\n",
    "\n",
    "            palette     = cycle(cm.tab20.colors)\n",
    "            colour_map  = {val: next(palette) for val in unique_vals}\n",
    "            missing_col = \"#d3d3d3\"                     # grey for None\n",
    "            node_colors = [colour_map.get(v, missing_col) for v in values]\n",
    "\n",
    "\n",
    "        for n in G.nodes():\n",
    "            val = G.nodes[n].get(node_attr)\n",
    "            node_labels[n] = f\"{n}\\n{node_attr}={val}\"\n",
    "\n",
    "    else:\n",
    "        node_labels = {n: n for n in G.nodes()}\n",
    "\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=400)\n",
    "    nx.draw_networkx_edges(G, pos, arrows=G.is_directed())\n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8)\n",
    "\n",
    "    '''\n",
    "    if edge_attr is not None:\n",
    "        edge_labels = {\n",
    "            (u, v): f\"{edge_attr}={d.get(edge_attr)}\"\n",
    "            for u, v, d in G.edges(data=True) if edge_attr in d\n",
    "        }\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "    '''\n",
    "\n",
    "    if node_attr and not all(isinstance(v, (int, float)) for v in values if v is not None):\n",
    "        handles = [\n",
    "            plt.Line2D([0], [0], marker=\"o\", color=\"w\",\n",
    "                       markerfacecolor=colour_map[val], markersize=8,\n",
    "                       label=str(val))\n",
    "            for val in unique_vals\n",
    "        ]\n",
    "        if any(v is None for v in values):\n",
    "            handles.append(\n",
    "                plt.Line2D([0], [0], marker=\"o\", color=\"w\",\n",
    "                           markerfacecolor=missing_col, markersize=8,\n",
    "                           label=\"None\")\n",
    "            )\n",
    "        plt.legend(title=node_attr, handles=handles,\n",
    "                   bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_node_index(json_data):\n",
    "    \"\"\"Map node name -> dict with nodeType and deviceName (from JSON only).\"\"\"\n",
    "    return {\n",
    "        n.get(\"name\"): {\n",
    "            \"nodeType\": n.get(\"nodeType\"),\n",
    "            \"deviceName\": n.get(\"deviceName\"),\n",
    "        }\n",
    "        for n in json_data.get(\"nodes\", [])\n",
    "    }\n",
    "\n",
    "def _parse_device_name(device_name):\n",
    "    \"\"\"\n",
    "    Parse a Cello device name like 'B3_BM3R1' -> ('B3', 'BM3R1').\n",
    "    Returns None for sensors/reporters/unset.\n",
    "    \"\"\"\n",
    "    if not device_name:\n",
    "        return None\n",
    "    dn = device_name.lower()\n",
    "    if \"sensor\" in dn or \"reporter\" in dn:\n",
    "        return None\n",
    "    parts = device_name.split(\"_\")\n",
    "    if len(parts) >= 2:\n",
    "        return parts[0].strip(), parts[-1].strip()\n",
    "    return None\n",
    "\n",
    "def assign_parts_from_json_repressors(\n",
    "    G: nx.DiGraph,\n",
    "    json_data: dict,\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    topological_sort: bool = False,\n",
    "    strict: bool = True,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Assign part parameters to interior (non-IO) nodes by reading each node's\n",
    "    deviceName from the Cello JSON and matching (RBS, Repressor) in `df`.\n",
    "\n",
    "    - Inputs/outputs tagged via node['type'] in {'input','output'}.\n",
    "    - Gate nodes get all columns from `df`, plus node['RBS'], node['Repressor'], node['type']='gate'.\n",
    "    - Removes any existing 'nodeType' and 'deviceName' attributes from nodes.\n",
    "    - Finally, relabels input nodes named in1, in2, ... to integers 0, 1, ...\n",
    "      (only those matching the pattern; others left unchanged).\n",
    "    - If verbose=True, prints detected IO, gate assignments, cleanup removals, and relabel mapping.\n",
    "    \"\"\"\n",
    "    H = G.copy()\n",
    "    jindex = _json_node_index(json_data)\n",
    "\n",
    "    # Identify IO via JSON; fall back to degree if missing.\n",
    "    input_nodes = {n for n, info in jindex.items() if info.get(\"nodeType\") == \"PRIMARY_INPUT\"}\n",
    "    output_nodes = {n for n, info in jindex.items() if info.get(\"nodeType\") == \"PRIMARY_OUTPUT\"}\n",
    "    if not input_nodes:\n",
    "        input_nodes = {n for n in H.nodes if H.in_degree(n) == 0}\n",
    "    if not output_nodes:\n",
    "        output_nodes = {n for n in H.nodes if H.out_degree(n) == 0}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== assign_parts_from_json_repressors: changes ===\")\n",
    "        print(f\"[detect] inputs:  {list(input_nodes)}\")\n",
    "        print(f\"[detect] outputs: {list(output_nodes)}\")\n",
    "\n",
    "    # Interior nodes\n",
    "    non_io = [n for n in H.nodes if n not in input_nodes and n not in output_nodes]\n",
    "    if topological_sort:\n",
    "        non_io = [n for n in nx.topological_sort(H) if n in non_io]\n",
    "    else:\n",
    "        non_io = sorted(non_io)\n",
    "\n",
    "    # Validate df & build lookup {(RBS, Repressor)->row}\n",
    "    if \"Repressor\" not in df.columns or \"RBS\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Repressor' and 'RBS' columns.\")\n",
    "    key_to_idx = {\n",
    "        (str(row[\"RBS\"]).upper().strip(), str(row[\"Repressor\"]).lower().strip()): i\n",
    "        for i, row in df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Tag IO types\n",
    "    for n in input_nodes:\n",
    "        H.nodes[n][\"type\"] = \"input\"\n",
    "    for n in output_nodes:\n",
    "        H.nodes[n][\"type\"] = \"output\"\n",
    "\n",
    "    # Assign gate params\n",
    "    for n in non_io:\n",
    "        device_name = (jindex.get(n) or {}).get(\"deviceName\")\n",
    "        parsed = _parse_device_name(device_name)\n",
    "        if parsed is None:\n",
    "            if strict:\n",
    "                raise ValueError(f\"Node '{n}' has unparseable or unsupported deviceName: {device_name}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"[skip ] {n}: unparseable/unsupported deviceName '{device_name}'\")\n",
    "                continue\n",
    "\n",
    "        rbs, repressor = parsed\n",
    "        key = (rbs.upper(), repressor.lower())\n",
    "        if key not in key_to_idx:\n",
    "            if strict:\n",
    "                raise ValueError(f\"No match in df for node '{n}' with (RBS='{rbs}', Repressor='{repressor}').\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"[skip ] {n}: no df match for (RBS='{rbs}', Repressor='{repressor}')\")\n",
    "                continue\n",
    "\n",
    "        idx = key_to_idx[key]\n",
    "        row = df.loc[idx]\n",
    "        for col in df.columns:\n",
    "            H.nodes[n][col] = row[col]\n",
    "        H.nodes[n][\"RBS\"] = rbs\n",
    "        H.nodes[n][\"Repressor\"] = repressor\n",
    "        H.nodes[n][\"type\"] = \"gate\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[assign] {n}: deviceName='{device_name}' -> RBS='{rbs}', Repressor='{repressor}' (df row {idx})\")\n",
    "\n",
    "    # Remove legacy attributes from ALL nodes (track removals)\n",
    "    removed_any = False\n",
    "    for n in H.nodes:\n",
    "        removed = []\n",
    "        if \"nodeType\" in H.nodes[n]:\n",
    "            H.nodes[n].pop(\"nodeType\", None)\n",
    "            removed.append(\"nodeType\")\n",
    "        if \"deviceName\" in H.nodes[n]:\n",
    "            H.nodes[n].pop(\"deviceName\", None)\n",
    "            removed.append(\"deviceName\")\n",
    "        if verbose and removed:\n",
    "            removed_any = True\n",
    "            print(f\"[cleanup] {n}: removed {removed}\")\n",
    "    if verbose and not removed_any:\n",
    "        print(\"[cleanup] no 'nodeType'/'deviceName' attributes were present on nodes\")\n",
    "\n",
    "    # === Relabel inputs: in1->0, in2->1, in3->2, in4->3 (ints) ===\n",
    "    pattern = re.compile(r'^(?:in|IN)(\\d+)$')\n",
    "    mapping = {}\n",
    "    for name in list(input_nodes):\n",
    "        m = pattern.match(str(name))\n",
    "        if not m:\n",
    "            continue  # only rename inX-style inputs\n",
    "        k = int(m.group(1))\n",
    "        new_label = k - 1  # zero-based: in1 -> 0\n",
    "        if new_label in H and new_label != name:\n",
    "            raise ValueError(f\"Cannot relabel '{name}' to {new_label}: label already exists.\")\n",
    "        mapping[name] = new_label\n",
    "\n",
    "    if mapping:\n",
    "        if verbose:\n",
    "            for old, new in mapping.items():\n",
    "                print(f\"[relabel] {old} -> {new}\")\n",
    "        H = nx.relabel_nodes(H, mapping, copy=True)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"[relabel] no input relabeling performed\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== end ===\")\n",
    "\n",
    "    return H\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "def assign_io_from_json(\n",
    "    G: nx.DiGraph,\n",
    "    json_data: dict,\n",
    "    *,\n",
    "    fallback_by_degree: bool = True,\n",
    "    relabel_inputs: bool = True,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tag IO nodes using Cello-like JSON (nodeType PRIMARY_INPUT/PRIMARY_OUTPUT), with\n",
    "    robust name matching and degree-based augmentation.\n",
    "\n",
    "    - Sets node['type'] = 'input' / 'output' for detected IO nodes.\n",
    "    - Leaves non-IO nodes' attributes untouched (no gate assignments).\n",
    "    - Cleans legacy attributes ('nodeType', 'deviceName') from ALL nodes.\n",
    "    - Optionally relabels input nodes named in1,in2,... to integers 0,1,2,... (zero-based).\n",
    "    - Returns a *copy* of G with updates.\n",
    "\n",
    "    Robustness:\n",
    "    - Accepts common JSON shapes: {\"nodes\":[...]}, {\"design\":{\"nodes\":[...]}}, or a\n",
    "      mapping-like dict of label->info.\n",
    "    - Name matching is case/whitespace tolerant (strip + lower).\n",
    "    - Fallback via degree **augments** JSON-detected IO (does not replace), so a single\n",
    "      missed input like 'in4' is still captured when it has in-degree==0.\n",
    "    \"\"\"\n",
    "\n",
    "    def _index_nodes(j):\n",
    "        \"\"\"Map: node_label -> node_json_dict for common JSON shapes.\"\"\"\n",
    "        idx = {}\n",
    "        if isinstance(j, dict):\n",
    "            if isinstance(j.get(\"nodes\"), list):\n",
    "                cand = j[\"nodes\"]\n",
    "            elif isinstance(j.get(\"design\"), dict) and isinstance(j[\"design\"].get(\"nodes\"), list):\n",
    "                cand = j[\"design\"][\"nodes\"]\n",
    "            elif j and all(isinstance(v, dict) for v in j.values()):\n",
    "                # Mapping of label->info\n",
    "                for k, v in j.items():\n",
    "                    idx[str(k)] = v\n",
    "                return idx\n",
    "            else:\n",
    "                cand = []\n",
    "            for nd in cand:\n",
    "                key = nd.get(\"name\") or nd.get(\"id\") or nd.get(\"label\") or nd.get(\"ref\") or nd.get(\"node\")\n",
    "                if key is not None:\n",
    "                    idx[str(key)] = nd\n",
    "        return idx\n",
    "\n",
    "    def _norm(x):\n",
    "        return str(x).strip().lower() if x is not None else None\n",
    "\n",
    "    H = G.copy()\n",
    "    jindex = _index_nodes(json_data)\n",
    "\n",
    "    # --- 1) Detect IO via JSON (normalize names to avoid case/space mismatches)\n",
    "    json_inputs_raw  = {k for k, info in jindex.items()\n",
    "                        if str(info.get(\"nodeType\", \"\")).upper() == \"PRIMARY_INPUT\"}\n",
    "    json_outputs_raw = {k for k, info in jindex.items()\n",
    "                        if str(info.get(\"nodeType\", \"\")).upper() == \"PRIMARY_OUTPUT\"}\n",
    "\n",
    "    json_inputs_norm  = {_norm(k) for k in json_inputs_raw}\n",
    "    json_outputs_norm = {_norm(k) for k in json_outputs_raw}\n",
    "\n",
    "    # Normalize graph labels once\n",
    "    label_norm = {n: _norm(n) for n in H.nodes}\n",
    "\n",
    "    # Start with JSON-detected IO that actually exist in the graph (by normalized name)\n",
    "    input_nodes  = {n for n, v in label_norm.items() if v in json_inputs_norm}\n",
    "    output_nodes = {n for n, v in label_norm.items() if v in json_outputs_norm}\n",
    "\n",
    "    # --- 2) Fallback via degree (AUGMENT, don't replace)\n",
    "    if fallback_by_degree:\n",
    "        deg_inputs  = {n for n in H.nodes if H.in_degree(n) == 0}\n",
    "        deg_outputs = {n for n in H.nodes if H.out_degree(n) == 0}\n",
    "\n",
    "        # Also include any string-named inputs like \"in4\" / \"IN 4\" etc.\n",
    "        p_named = re.compile(r'^(?:in|IN)[ _]?(\\d+)$')\n",
    "        named_inputs = {n for n in H.nodes if isinstance(n, str) and p_named.match(n)}\n",
    "\n",
    "        input_nodes  |= (deg_inputs | named_inputs)\n",
    "        output_nodes |= deg_outputs\n",
    "\n",
    "        # Ensure disjointness: an isolated node shouldn't be both\n",
    "        output_nodes -= input_nodes\n",
    "\n",
    "    if verbose:\n",
    "        def show(x): return repr(x)  # reveals trailing spaces if any\n",
    "        print(\"=== assign_io_from_json: changes ===\")\n",
    "        print(f\"[detect] JSON inputs (raw):  {sorted(map(show, json_inputs_raw))}\")\n",
    "        print(f\"[detect] JSON outputs (raw): {sorted(map(show, json_outputs_raw))}\")\n",
    "        print(f\"[detect] inputs (final):  {sorted(map(show, input_nodes), key=str)}\")\n",
    "        print(f\"[detect] outputs (final): {sorted(map(show, output_nodes), key=str)}\")\n",
    "        if fallback_by_degree:\n",
    "            print(f\"[debug ] deg-in==0:  {sorted(map(show, (n for n in H.nodes if H.in_degree(n)==0)), key=str)}\")\n",
    "            print(f\"[debug ] deg-out==0: {sorted(map(show, (n for n in H.nodes if H.out_degree(n)==0)), key=str)}\")\n",
    "\n",
    "    # --- 3) Tag IO types\n",
    "    for n in input_nodes:\n",
    "        H.nodes[n][\"type\"] = \"input\"\n",
    "    for n in output_nodes:\n",
    "        H.nodes[n][\"type\"] = \"output\"\n",
    "\n",
    "    # --- 4) Clean up legacy per-node attributes everywhere\n",
    "    removed_any = False\n",
    "    for n in H.nodes:\n",
    "        removed = []\n",
    "        if \"nodeType\" in H.nodes[n]:\n",
    "            H.nodes[n].pop(\"nodeType\", None)\n",
    "            removed.append(\"nodeType\")\n",
    "        if \"deviceName\" in H.nodes[n]:\n",
    "            H.nodes[n].pop(\"deviceName\", None)\n",
    "            removed.append(\"deviceName\")\n",
    "        if verbose and removed:\n",
    "            removed_any = True\n",
    "            print(f\"[cleanup] {n!r}: removed {removed}\")\n",
    "    if verbose and not removed_any:\n",
    "        print(\"[cleanup] no 'nodeType'/'deviceName' attributes were present on nodes\")\n",
    "\n",
    "    # --- 5) Optional: relabel inputs in1->0, in2->1, ... (zero-based)\n",
    "    if relabel_inputs and input_nodes:\n",
    "        p_relabel = re.compile(r'^(?:in|IN)[ _]?(\\d+)$')\n",
    "        mapping = {}\n",
    "        for name in list(input_nodes):\n",
    "            if not isinstance(name, str):\n",
    "                continue\n",
    "            m = p_relabel.match(name)\n",
    "            if not m:\n",
    "                continue\n",
    "            new_label = int(m.group(1)) - 1  # zero-based: in1 -> 0\n",
    "            if new_label in H and new_label != name:\n",
    "                raise ValueError(f\"Cannot relabel '{name}' to {new_label}: label already exists.\")\n",
    "            mapping[name] = new_label\n",
    "\n",
    "        if mapping:\n",
    "            if verbose:\n",
    "                for old, new in sorted(mapping.items(), key=lambda x: str(x[0])):\n",
    "                    print(f\"[relabel] {old!r} -> {new!r}\")\n",
    "            H = nx.relabel_nodes(H, mapping, copy=True)\n",
    "        elif verbose:\n",
    "            print(\"[relabel] no input relabeling performed\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== end ===\")\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _outval(v):\n",
    "    \"\"\"Accept (0,), [0], or 0 -> int 0/1.\"\"\"\n",
    "    if isinstance(v, (tuple, list)):\n",
    "        return int(v[0])\n",
    "    return int(v)\n",
    "\n",
    "def _seq_from_sim(sim_list):\n",
    "    \"\"\"[{'out':0}, ...] or [0,1,...] -> [0/1,...].\"\"\"\n",
    "    return [\n",
    "        int(next(iter(d.values()))) if isinstance(d, dict) else int(d)\n",
    "        for d in sim_list\n",
    "    ]\n",
    "\n",
    "def _seq_from_tt(tt_dict):\n",
    "    \"\"\"Ordered dict of {(a,b,c): (0,), ...} -> [0/1,...] using current insertion order.\"\"\"\n",
    "    return [_outval(v) for v in tt_dict.values()]\n",
    "\n",
    "def _bits_from_hex(hex_str, nrows, msb_first=True):\n",
    "    \"\"\"Return nrows bits from hex string; no reordering beyond MSB/LSB choice.\"\"\"\n",
    "    i = int(hex_str, 16)\n",
    "    s = f\"{i:0{nrows}b}\"\n",
    "    bits = [int(ch) for ch in s]\n",
    "    return bits if msb_first else bits[::-1]\n",
    "\n",
    "def _print_table(headers, rows):\n",
    "    widths = [max(len(str(h)), max((len(str(r[i])) for r in rows), default=0))\n",
    "              for i, h in enumerate(headers)]\n",
    "    fmt = \" | \".join(f\"{{:{w}}}\" for w in widths)\n",
    "    sep = \"-+-\".join(\"-\" * w for w in widths)\n",
    "    print(fmt.format(*headers))\n",
    "    print(sep)\n",
    "    for r in rows:\n",
    "        print(fmt.format(*r))\n",
    "\n",
    "\n",
    "import math\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "def report_truth_table_ordered(\n",
    "    design_name,\n",
    "    sim_result  = None,\n",
    "    tt1 = None,\n",
    "    tt2  = None,\n",
    "    hex_str = None,\n",
    "    input_names = None,\n",
    "    msb_first_hex  = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Print a row-aligned comparison table using whichever sources are provided\n",
    "    (sim_result, tt1, tt2, hex_str), and return summary stats.\n",
    "\n",
    "    Assumptions:\n",
    "    - All provided sources are already aligned row-by-row (same order).\n",
    "    - If tt1 or tt2 is provided and its keys are tuples, that defines the row labels.\n",
    "    - If no TT is provided, row labels are generated from nrows if it's a power of two;\n",
    "      otherwise row indices are used.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        'design': str,\n",
    "        'nrows': int,\n",
    "        'n_compared_rows': int,     # rows where >=2 sources were present\n",
    "        'n_ok': int,                # rows where all present sources agree\n",
    "        'pct_ok': float | None,     # None if n_compared_rows == 0\n",
    "        'all_equal': bool | None,   # None if n_compared_rows == 0\n",
    "        'columns': [str,...],       # which columns were included\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- helpers (use your existing _seq_from_sim/_seq_from_tt/_print_table) ----\n",
    "    def _norm_hex_digits(s: str) -> str:\n",
    "        \"\"\"Extract leading hex run after optional 0x prefix; tolerant of suffixes.\"\"\"\n",
    "        m = re.match(r'^\\s*(?:0[xX])?([0-9A-Fa-f]+)', str(s))\n",
    "        if not m:\n",
    "            raise ValueError(f\"Couldn't parse hex digits from {s!r}\")\n",
    "        return m.group(1)\n",
    "\n",
    "    def _bits_from_hex_auto(hex_str_local, nrows_hint, msb_first=True):\n",
    "        \"\"\"Return (bits, nrows_used). If nrows_hint is None, derive nrows from hex length.\"\"\"\n",
    "        hexdigits = _norm_hex_digits(hex_str_local)\n",
    "        i = int(hexdigits, 16)\n",
    "        if nrows_hint is None:\n",
    "            nrows_used = 4 * len(hexdigits)  # each hex nibble = 4 bits\n",
    "        else:\n",
    "            nrows_used = nrows_hint\n",
    "        # Safety: don't overflow; left-pad to nrows_used\n",
    "        bitlen = max(1, i.bit_length())\n",
    "        if bitlen > nrows_used:\n",
    "            raise ValueError(\n",
    "                f\"Hex '{hexdigits}' encodes {bitlen} bits but expected <= {nrows_used}.\"\n",
    "            )\n",
    "        s = f\"{i:0{nrows_used}b}\"\n",
    "        bits = [int(ch) for ch in s]\n",
    "        return (bits if msb_first else bits[::-1], nrows_used)\n",
    "\n",
    "    # ---- figure out nrows from whichever sources are present ----\n",
    "    candidates = set()\n",
    "    if tt1 is not None:\n",
    "        candidates.add(len(tt1))\n",
    "    if tt2 is not None:\n",
    "        candidates.add(len(tt2))\n",
    "    if sim_result is not None:\n",
    "        candidates.add(len(sim_result))\n",
    "    nrows_hex = None\n",
    "    if hex_str is not None:\n",
    "        nrows_hex = 4 * len(_norm_hex_digits(hex_str))\n",
    "        candidates.add(nrows_hex)\n",
    "\n",
    "    if not candidates:\n",
    "        # nothing to compare; print a stub and return\n",
    "        print(f\"\\n=== {design_name} ===\")\n",
    "        print(\"(No sources provided: sim_result/tt1/tt2/hex_str are all None)\")\n",
    "        return {\n",
    "            \"design\": design_name,\n",
    "            \"nrows\": 0,\n",
    "            \"n_compared_rows\": 0,\n",
    "            \"n_ok\": 0,\n",
    "            \"pct_ok\": None,\n",
    "            \"all_equal\": None,\n",
    "            \"columns\": [],\n",
    "        }\n",
    "\n",
    "    if len(candidates) > 1:\n",
    "        raise ValueError(f\"Inconsistent row counts across sources: {sorted(candidates)}\")\n",
    "    nrows = candidates.pop()\n",
    "\n",
    "    # ---- build row labels ----\n",
    "    # Prefer tuple keys from a provided truth table (tt1 then tt2)\n",
    "    input_rows = None\n",
    "    if tt1 is not None:\n",
    "        keys = list(tt1.keys())\n",
    "        if keys and isinstance(keys[0], tuple):\n",
    "            input_rows = keys\n",
    "    if input_rows is None and tt2 is not None:\n",
    "        keys = list(tt2.keys())\n",
    "        if keys and isinstance(keys[0], tuple):\n",
    "            input_rows = keys\n",
    "\n",
    "    # If no tuple labels, try to synthesize from nrows (if power of two),\n",
    "    # with A as MSB, D as LSB, etc.\n",
    "    n_inputs = None\n",
    "    if input_rows is None:\n",
    "        # Check if nrows is a power of two\n",
    "        if nrows > 0 and (nrows & (nrows - 1)) == 0:\n",
    "            n_inputs = int(math.log2(nrows))\n",
    "            # A is most significant (slowest-changing)\n",
    "            input_rows = [tuple(p) for p in product([0, 1], repeat=n_inputs)]\n",
    "        else:\n",
    "            # Fall back to plain indices\n",
    "            input_rows = list(range(nrows))\n",
    "\n",
    "    if n_inputs is None:\n",
    "        # If we used tuple labels, infer arity\n",
    "        if input_rows and isinstance(input_rows[0], tuple):\n",
    "            n_inputs = len(input_rows[0])\n",
    "        else:\n",
    "            n_inputs = 0  # unknown / not tuple-style\n",
    "\n",
    "    # ---- gather column data only for provided sources ----\n",
    "    columns = []     # names for headers\n",
    "    col_values = []  # list of lists, each length == nrows\n",
    "\n",
    "    if sim_result is not None:\n",
    "        sim_vals = _seq_from_sim(sim_result)\n",
    "        if len(sim_vals) != nrows:\n",
    "            raise ValueError(f\"sim_result length {len(sim_vals)} != expected nrows {nrows}\")\n",
    "        columns.append(\"Sim\")\n",
    "        col_values.append(sim_vals)\n",
    "\n",
    "    if tt1 is not None:\n",
    "        tt1_vals = _seq_from_tt(tt1)\n",
    "        if len(tt1_vals) != nrows:\n",
    "            raise ValueError(f\"tt1 length {len(tt1_vals)} != expected nrows {nrows}\")\n",
    "        columns.append(\"TT1\")\n",
    "        col_values.append(tt1_vals)\n",
    "\n",
    "    if tt2 is not None:\n",
    "        tt2_vals = _seq_from_tt(tt2)\n",
    "        if len(tt2_vals) != nrows:\n",
    "            raise ValueError(f\"tt2 length {len(tt2_vals)} != expected nrows {nrows}\")\n",
    "        columns.append(\"TT2\")\n",
    "        col_values.append(tt2_vals)\n",
    "\n",
    "    if hex_str is not None:\n",
    "        hex_vals, nrows_used = _bits_from_hex_auto(hex_str, nrows_hint=nrows, msb_first=msb_first_hex)\n",
    "        if len(hex_vals) != nrows:\n",
    "            raise ValueError(f\"hex length {len(hex_vals)} != expected nrows {nrows}\")\n",
    "        columns.append(\"Hex\")\n",
    "        col_values.append(hex_vals)\n",
    "\n",
    "    # ---- headers ----\n",
    "    if input_names is None and n_inputs and n_inputs > 0:\n",
    "        input_names = [chr(ord(\"A\") + i) for i in range(n_inputs)]\n",
    "    elif input_names is None:\n",
    "        input_names = []  # no input columns if we don't have tuple labels\n",
    "\n",
    "    headers = list(input_names) + columns + ([\"Row OK?\"] if len(columns) >= 2 else [])\n",
    "\n",
    "    # ---- build rows & OK mask ----\n",
    "    n_compared_rows = 0\n",
    "    n_ok = 0\n",
    "    rows = []\n",
    "    for i in range(nrows):\n",
    "        # Input label cells\n",
    "        if isinstance(input_rows[i], tuple):\n",
    "            row_cells = list(input_rows[i])\n",
    "        else:\n",
    "            row_cells = [input_rows[i]] if input_names else []\n",
    "\n",
    "        # Col values for this row\n",
    "        vals_i = [col[i] for col in col_values]\n",
    "\n",
    "        # Decide OK? (only if we have >=2 sources)\n",
    "        ok_cell = None\n",
    "        if len(vals_i) >= 2:\n",
    "            n_compared_rows += 1\n",
    "            if all(v == vals_i[0] for v in vals_i[1:]):\n",
    "                ok_cell = \"OK\"\n",
    "                n_ok += 1\n",
    "            else:\n",
    "                ok_cell = \"DIFF\"\n",
    "\n",
    "        row_cells.extend(vals_i)\n",
    "        if ok_cell is not None:\n",
    "            row_cells.append(ok_cell)\n",
    "\n",
    "        rows.append(row_cells)\n",
    "\n",
    "    pct_ok = (100.0 * n_ok / n_compared_rows) if n_compared_rows else None\n",
    "    all_equal = (n_ok == n_compared_rows) if n_compared_rows else None\n",
    "\n",
    "    # ---- print ----\n",
    "    print(f\"\\n=== {design_name} ===\")\n",
    "    _print_table(headers, rows)\n",
    "    if n_compared_rows:\n",
    "        print(f\"Agreement for {design_name}: {n_ok}/{n_compared_rows} rows = {pct_ok:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Agreement for {design_name}: N/A (need at least two sources)\")\n",
    "\n",
    "    return {\n",
    "        \"design\": design_name,\n",
    "        \"nrows\": nrows,\n",
    "        \"n_compared_rows\": n_compared_rows,\n",
    "        \"n_ok\": n_ok,\n",
    "        \"pct_ok\": pct_ok,\n",
    "        \"all_equal\": all_equal,\n",
    "        \"columns\": columns,\n",
    "    }\n",
    "    \n",
    "def _detect_output_nodes(G):\n",
    "    # 1) Prefer nodes tagged as outputs\n",
    "    outs = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"output\"]\n",
    "    # 2) Otherwise prefer a node literally named 'out'\n",
    "    if not outs and \"out\" in G:\n",
    "        outs = [\"out\"]\n",
    "    # 3) Otherwise, use nodes with out-degree == 0\n",
    "    if not outs:\n",
    "        outs = [n for n in G.nodes if G.out_degree(n) == 0]\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_non_int_nodes_to_int(\n",
    "    G,\n",
    "    *,\n",
    "    start_at = None,\n",
    "    in_place = False,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Relabel all non-integer-labeled nodes in `G` to fresh integer IDs that do not\n",
    "    collide with existing integer node labels. Works for Graph/DiGraph/Multi*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : any NetworkX graph\n",
    "    start_at : optional int\n",
    "        Lowest candidate ID to use. If None, uses max(existing_ints, default=-1) + 1.\n",
    "    in_place : bool\n",
    "        If True, relabel in place; otherwise a new graph is returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H : NetworkX graph (same type as G)\n",
    "    mapping : dict\n",
    "        {old_label -> new_int_label} for nodes that were changed.\n",
    "    \"\"\"\n",
    "    def is_int_like(x) -> bool:\n",
    "        # Treat numpy integers as ints; booleans count as ints in Python (True==1, False==0)\n",
    "        return isinstance(x, numbers.Integral)\n",
    "\n",
    "    existing_ints = {n for n in G.nodes if is_int_like(n)}\n",
    "    non_int_nodes = [n for n in G.nodes if not is_int_like(n)]\n",
    "\n",
    "    # Choose a starting point that won't collide\n",
    "    if start_at is None:\n",
    "        nxt = (max(existing_ints) + 1) if existing_ints else 0\n",
    "    else:\n",
    "        nxt = start_at\n",
    "\n",
    "    used = set(existing_ints)\n",
    "    mapping: Dict[Hashable, int] = {}\n",
    "\n",
    "    for n in non_int_nodes:\n",
    "        while nxt in used:\n",
    "            nxt += 1\n",
    "        mapping[n] = nxt\n",
    "        used.add(nxt)\n",
    "        nxt += 1\n",
    "\n",
    "    if not mapping:\n",
    "        return (G if in_place else G.copy()), mapping\n",
    "\n",
    "    H = nx.relabel_nodes(G, mapping, copy=not in_place)\n",
    "    return H, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates Verilog for all 3-input 1-output circuits (except 00 and FF)\n",
    "with the Wolfram naming scheme. You probably don't need to run this.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'Timothy S. Jones <jonests@bu.edu>, Densmore Lab, BU'\n",
    "__license__ = 'GPL3'\n",
    "\n",
    "import os\n",
    "\n",
    "verilog = \"\"\"module m0x{name}(output out, input in1, in2, in3);\n",
    "\n",
    "   always @(in1, in2, in3)\n",
    "     begin\n",
    "        case({{in1, in2, in3}})\n",
    "          3'b000: {{out}} = 1'b{b[0]};\n",
    "          3'b001: {{out}} = 1'b{b[1]};\n",
    "          3'b010: {{out}} = 1'b{b[2]};\n",
    "          3'b011: {{out}} = 1'b{b[3]};\n",
    "          3'b100: {{out}} = 1'b{b[4]};\n",
    "          3'b101: {{out}} = 1'b{b[5]};\n",
    "          3'b110: {{out}} = 1'b{b[6]};\n",
    "          3'b111: {{out}} = 1'b{b[7]};\n",
    "        endcase // case ({{in1, in2, in3}})\n",
    "     end // always @ (in1, in2, in3)\n",
    "\n",
    "endmodule // m0x{name}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23ecda",
   "metadata": {},
   "source": [
    "#### Code for importing a Cello 2.0 design (3 input) into DESIGNR and score it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf680c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the JSON file\n",
    "json_file_path = '/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/dgd/data/Cello_2_designs/0x000D_V2/0x000D_V2_outputNetlist.json'\n",
    "\n",
    "\n",
    "# Load JSON data from file\n",
    "json_data = load_json_file(json_file_path)\n",
    "\n",
    "# Create a DAG from the JSON data\n",
    "dag = create_dag_from_json(json_data)\n",
    "\n",
    "# Print the nodes and edges in the DAG\n",
    "print(\"Nodes in the DAG:\")\n",
    "for node in dag.nodes(data=True):\n",
    "    print(node)\n",
    "\n",
    "print(\"\\nEdges in the DAG:\")\n",
    "for edge in dag.edges(data=True):\n",
    "    print(edge)\n",
    "    \n",
    "G = dag\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "pos = nx.spring_layout(G)  # Positions for all nodes\n",
    "nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=100, \n",
    "        edge_color='k', linewidths=1, font_size=8, \n",
    "        arrows=True, arrowsize=10)\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1936ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_assigned = assign_io_from_json(G, json_data, verbose = True)\n",
    "        \n",
    "for attr in G_assigned.nodes(data=True):\n",
    "    print(attr)\n",
    "\n",
    "topology_plot_with_attrs(G_assigned, seed = 1459)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5b8a1",
   "metadata": {},
   "source": [
    "#### Code for analyzing a 3-input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/dgd/data/Cello_2_designs_3_input'\n",
    "\n",
    "cello_designs_gates_assigned_3_input = {}\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\"_outputNetlist.json\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {full_path}\")  # Print the file being processed\n",
    "            data = load_json_file(full_path)\n",
    "            graph = create_dag_from_json(data)\n",
    "            \n",
    "            nodes_with_no_incoming_edges = [n for n in graph.nodes() if graph.in_degree(n) == 0]\n",
    "            \n",
    "            if len(nodes_with_no_incoming_edges) != 3:\n",
    "                print(\"[INFO] Design does not have 3 inputs\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                #graph_parts_assigned = assign_parts_from_json_repressors(graph, data, cello_v1_hill_function_parameters, topological_sort=False, verbose=False, strict=False)\n",
    "                graph_parts_assigned = assign_io_from_json(graph, json_data, verbose = False)\n",
    "            except ValueError:\n",
    "                print(\"Value Error\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Unexpected error while assigning parts: {e}\")\n",
    "                continue           \n",
    "            \n",
    "            # Use the directory name as the key for the graph\n",
    "            dir_name = os.path.basename(root)\n",
    "            #topology_plot_with_attrs(graph_parts_assigned, seed = 1459)\n",
    "            cello_designs_gates_assigned_3_input[dir_name] = graph_parts_assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cello_designs_gates_assigned_3_input))\n",
    "cello_designs_gates_assigned_3_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tralsform all node labels to int \n",
    "    \n",
    "for cello_circuit_hex, design in cello_designs_gates_assigned_3_input.items():\n",
    "    H, mapping = relabel_non_int_nodes_to_int(design, in_place=True)\n",
    "    print(f'{H}, {mapping}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da743fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for cello_circuit_hex, design in cello_designs_gates_assigned_3_input.items():\n",
    "    \n",
    "    fanin_size = 2\n",
    "    #version 1:    \n",
    "    G = design.copy()\n",
    "    if G.number_of_nodes() == 0:\n",
    "        raise ValueError(\"Graph must contain at least one node\")\n",
    "\n",
    "    # 1) Identify primary outputs (sink nodes)\n",
    "    output_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "    if not output_nodes:\n",
    "        raise ValueError(\"Graph has no sink/output nodes\")\n",
    "\n",
    "    # 2) Identify primary inputs (source nodes)\n",
    "    input_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "\n",
    "    # 3) Detect implicit‑OR opportunities anchored at the first output\n",
    "    max_removal = 0\n",
    "    best_key = None\n",
    "    implicit_or_results = check_implicit_OR_existence_v3(G, output_nodes[0], fanin_size)\n",
    "\n",
    "    for key, val in implicit_or_results.items():\n",
    "        if val.get(\"is_there_an_implicit_OR\", False) and val.get(\"number_of_nodes_available_for_removal\", 0) > max_removal:\n",
    "            max_removal = val[\"number_of_nodes_available_for_removal\"]\n",
    "            best_key = key               \n",
    "    \n",
    "    print(f'version 1: {max_removal}, {best_key}')\n",
    "            \n",
    "    #verstion 2: \n",
    "    G_copy = G.copy()\n",
    "    output_nodes = [n for n in G_copy if G_copy.out_degree(n) == 0]\n",
    "    output_node = output_nodes[0]\n",
    "    results_check_implicit_OR_existence = check_implicit_OR_existence_v3(G_copy, output_node, fanin_size)\n",
    "    best_node_reduction_found, best_node_reduction_found_key = 0, None\n",
    "    for key, value in results_check_implicit_OR_existence.items():\n",
    "        if value[\"is_there_an_implicit_OR\"] and value[\"number_of_nodes_available_for_removal\"] > best_node_reduction_found:\n",
    "            best_node_reduction_found_key, best_node_reduction_found = key, value[\"number_of_nodes_available_for_removal\"]\n",
    "\n",
    "    print(f'version 2: {best_node_reduction_found}, {best_node_reduction_found_key}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c1ceb",
   "metadata": {},
   "source": [
    "Code for loading a registry and testing with those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_registry(pkl_file):\n",
    "    \"\"\"Load the pickle and rebuild NetworkX graphs.\"\"\"\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        saved = pickle.load(f)\n",
    "\n",
    "    registry = {}\n",
    "    for h, bucket in saved.items():\n",
    "        restored = []\n",
    "        for canon_nl, orig_nl, e in bucket:\n",
    "            canon = nx.node_link_graph(canon_nl)\n",
    "            orig  = nx.node_link_graph(orig_nl)\n",
    "            restored.append((canon, orig, e))\n",
    "        registry[h] = restored\n",
    "    return registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c03c8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl_path = \"/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/scripts/runs/Fig3_4input_200_logic_functions_registry_sampling_drl3env_loader5/seed_1/shared_registry_25_600.pkl\"\n",
    "\n",
    "pkl_path = '/home/gridsan/spalacios/Designing complex biological circuits with deep neural networks/scripts/runs/Fig3_4input_4000_logic_functions_registry_sampling_drl3env_loader5_v2/seed_1/shared_registry_6_400.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8803739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry  = load_registry(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e16c2ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10147"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b379f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_1_results = []\n",
    "version_2_results = []\n",
    "\n",
    "for key, bucket in registry.items():\n",
    "    \n",
    "    g_original = bucket[0][1]\n",
    "    fanin_size = 2\n",
    "    #version 1:    \n",
    "    G = g_original.copy()\n",
    "    if G.number_of_nodes() == 0:\n",
    "        raise ValueError(\"Graph must contain at least one node\")\n",
    "\n",
    "    # 1) Identify primary outputs (sink nodes)\n",
    "    output_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
    "    if not output_nodes:\n",
    "        raise ValueError(\"Graph has no sink/output nodes\")\n",
    "\n",
    "    # 2) Identify primary inputs (source nodes)\n",
    "    input_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "\n",
    "    # 3) Detect implicit‑OR opportunities anchored at the first output\n",
    "    max_removal = 0\n",
    "    best_key = None\n",
    "    implicit_or_results = check_implicit_OR_existence_v3(G, output_nodes[0], fanin_size)\n",
    "\n",
    "    for key, val in implicit_or_results.items():\n",
    "        if val.get(\"is_there_an_implicit_OR\", False) and val.get(\"number_of_nodes_available_for_removal\", 0) > max_removal:\n",
    "            max_removal = val[\"number_of_nodes_available_for_removal\"]\n",
    "            best_key = key               \n",
    "    \n",
    "    #print(f'version 1: {max_removal}, {best_key}')\n",
    "    version_1_results.append((max_removal, best_key))\n",
    "            \n",
    "    #verstion 2: \n",
    "    G_copy = G.copy()\n",
    "    output_nodes = [n for n in G_copy if G_copy.out_degree(n) == 0]\n",
    "    output_node = output_nodes[0]\n",
    "    results_check_implicit_OR_existence = check_implicit_OR_existence_v3(G_copy, output_node, fanin_size)\n",
    "    best_node_reduction_found, best_node_reduction_found_key = 0, None\n",
    "    for key, value in results_check_implicit_OR_existence.items():\n",
    "        if value[\"is_there_an_implicit_OR\"] and value[\"number_of_nodes_available_for_removal\"] > best_node_reduction_found:\n",
    "            best_node_reduction_found_key, best_node_reduction_found = key, value[\"number_of_nodes_available_for_removal\"]\n",
    "\n",
    "    #print(f'version 2: {best_node_reduction_found}, {best_node_reduction_found_key}') \n",
    "    version_2_results.append((best_node_reduction_found, best_node_reduction_found_key))\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "351ef21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_1_results == version_2_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
